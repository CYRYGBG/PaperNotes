# FROM EMPIRICAL RISK MINIMIZATION TO mixup

## 背景

假设一个数据集中的特征向量是$X$和标签向量是$Y$（one-hot vector，示例如下表格），对应的一个观测（一个样本）就是服从联合概率分布$P(X, Y)$的$(x,y)$。在监督学习中，我们希望找到一个函数$f$将$x$映射到$y$，也就是用来描述输入$X$和$Y$之间的关系，同时也是之前那些分类模型比如CNN或者EEG Conformer的一个抽象表示。

| Apple | Chicken | Broccoli |
| :---: | :-----: | :------: |
|   1   |    0    |    0     |
|   0   |    1    |    0     |
|   0   |    0    |    1     |

现在我们通过损失函数$\ell$衡量预测值$f(x)$与真实值$y$的误差。理想情况下，我们希望我们找到的函数$f$是可以最小化**风险期望（expected risk）**的，可以描述成下面的公式：
$$
R(f) = \int \ell(f(x), y) \, \mathrm{d}P(x, y),
$$

但实际上，大部分情况下我们无法直接获取这个概率分布$P(X, Y)$，只能从这个分布中获取一些样本来构成有限的数据集$\mathcal{D} = \{(x_i, y_i)\}_{i=1}^n$。因此，在这里引入**经验分布（empirical distribution）**：

$$
P_\delta(x, y) = \frac{1}{n} \sum_{i=1}^n \delta(x = x_i, y = y_i)
$$

这里的$\delta(x = x_i, y = y_i)$定义为仅在点$(x_i,y_i)$处取值非零（理论上为无穷大），其他位置为零，同时，它在整个联合分布上积分为1，即
$$
\int\delta(x=x_i,y=y_i)dxdy=1,
$$
基于此，前面的期望风险就可以近似为**经验风险（empirical risk）**：
$$
R_\delta(f) = \int\ell(f(x),y)\mathrm{d}P_\delta(x,y)=\frac1n\sum_{i=1}^n\ell(f(x_i),y_i),
$$

这也就构成了一个机器学习模型训练时所使用的损失函数，意味着我们通过降低样本上的平均损失来对模型进行优化。

**经验风险最小化（Empirical Risk Minimization, ERM）**是监督学习的核心，通过最小化$R_\delta(f) $选择最优的函数（模型）$f$。但 ERM 有以下局限性：

- **有限样本限制**：ERM 仅基于训练集的$n$个样本，无法直接评估模型在数据分布$P$上的表现。
- **过拟合问题**：当模型参数量过大时，模型倾向于“记住”训练数据，导致在新数据上表现差。
- **对抗样本敏感**：ERM 训练的模型对微小扰动不稳定，稳定性不足。

这些问题表明，ERM 在高复杂度和大数据需求的深度学习中不够理想。另外，上面这种损失函数中采用的$P_\delta(x, y)$也只是众多对$P(X, Y)$近似的一种，下面要介绍的就是另一种近似方法。

为解决 ERM 的局限性，提出了**邻域风险最小化（Vicinal Risk Minimization, VRM）**。VRM 通过在训练样本“邻域”内生成虚拟样本，扩展数据覆盖范围，也就是数据增强，图像分类中对训练集的裁切宣传等操作也可以归为这一类。

这里对$P(X, Y)$的近似表示为：
$$
P_\nu(\tilde{x}, \tilde{y}) = \frac{1}{n} \sum_{i=1}^n \nu(\tilde{x}, \tilde{y} \mid x_i, y_i)
$$

其中的$(x_i,y_i)$是训练集中一个已知样本的特征向量和标签向量，$(\tilde{x}, \tilde{y})$是在已知样本邻域内生成的虚拟样本，那么$\nu(\tilde{x}, \tilde{y} \mid x_i, y_i)$是一个条件概率密度函数，表示在给定训练样本$(x_i,y_i)$的情况下，虚拟样本$(\tilde{x}, \tilde{y})$的分布，它定定义了如何基于一个真实的训练样本来生成一组“附近”的虚拟样本。

举例来说，如果使用高斯分布来在真实样本周围生成虚拟样本，对应的表达公式为：
$$
\nu(\tilde{x},\tilde{y}|x_{i},y_{i})=\mathcal{N}(\tilde{x}-x_{i},\sigma^{2})\delta(\tilde{y}=y_{i}),
$$
这里的噪声强度由$\sigma$进行控制，由$\delta(\tilde{y}=y_{i})$函数确保虚拟标签必须等于真实标签，应该可以理解为只是往数据集中样本的特征向量中加入了扰动，标签不会发生改变。

基于这个方法可以生成虚拟数据集$\mathcal{D}_\nu = \{(\tilde{x}_i, \tilde{y}_i)\}_{i=1}^m$，并定义**经验邻域风险**：
$$
R_\nu(f) = \frac{1}{m} \sum_{i=1}^m \ell(f(\tilde{x}_i), \tilde{y}_i)
$$

VRM 通过最小化$R_\nu(f)$鼓励模型学习更平滑的模式。

## mixup方法

论文认为，不改变样本标签的样本生成方法无法帮助模型学习不同类别样本之间的关系，所以提出了**mixup**方法， 是 VRM 的一种特殊实现，其**邻域分布**为：
$$
\mu(\tilde{x}, \tilde{y} \mid x_i, y_i) = \frac{1}{n} \sum_{j=1}^n \mathbb{E}_{\lambda} \left[ \delta\left( \tilde{x} = \lambda x_i + (1 - \lambda) x_j, \tilde{y} = \lambda y_i + (1 - \lambda) y_j \right) \right]
$$

和前面类似，这里表示的是给定训练样本$(x_i,y_i)$的情况下生成虚拟样本$(\tilde{x}, \tilde{y})$的概率，$\lambda\sim\text{Beta}(\alpha,\alpha)$，$\alpha\in(0,\infty)$。关于$\lambda$期望公式中则具体计算当前样本$(x_i,y_i)$和样本$(x_j,y_j)$的插值结果。

简单来说，mixup生成虚拟样本的方法就是随机抽取两个样本，然后从设置好的beta分布中获取$\lambda$的值作为两个样本的权重来计算生成一个虚拟样本
$$
\tilde{x}=\lambda x_i+(1-\lambda)x_j,
$$

$$
\tilde{y}=\lambda y_i+(1-\lambda)y_j,
$$

这里$\lambda\in[0,1]$。这里的对标签向量的融合可以按照前面独热编码的例子理解为如下表格：

| Apple | Chicken | \lambda Apple+ (1+ \lambda)Chicken |
| :---: | :-----: | :--------------------------------: |
|   1   |    0    |              \lambda               |
|   0   |    1    |             1+ \lambda             |
|   0   |    0    |                 0                  |

可以看到，和前面的VRM的方法最大的区别就是这个数据增强方法所生成的虚拟样本在标签上也会发生变化。对应的，控制这个插值强度的就是beta分布中的$\alpha$参数，当$\alpha\to0$时，这个VRM方法就变回了ERM。

|        | ERM                | VRM(传统数据增强)                                | Mixup                        |
| ------ | ------------------ | ------------------------------------------------ | ---------------------------- |
| 数据集 | 直接使用现有数据集 | 在现有数据集的样本邻域内生成新的数据，不改变标签 | 跨标签插值生成混合样本和标签 |
|        |                    |                                                  |                              |
|        |                    |                                                  |                              |

论文认为，这个方法鼓励模型以线性的方法表示不同类比的样本，这有利于增强模型在遇到新样本时的稳定性，也符合奥卡姆剃刀原则。如下图所示，论文认为mixup方法可以让决策边界以类线性的方式从一个类过渡到另一个类，提供了更平滑的不确定性估计。

![分类边界](https://gitee.com/fbanhua/figurebed/raw/master/images/20250328144326937.png)

这个方法在工程上的优点在于**实现简单**和**计算开销小**，论文指出仅需要插入几行代码就能实现这个数据增强方法。

![实现简单](https://gitee.com/fbanhua/figurebed/raw/master/images/20250328121551059.png)

随后，论文基于相同的模型结构和训练方法对ERM方法和mixup方法进行了对比，得到了如下图所示的结果。在图（a）中，当模型预测的类别不属于生成虚拟样本的两个样本之间的任一类别时，则把这次预测视作错误，可以看到，使用mixup方法的错误率明显低于ERM方法，特别是特征权重$\lambda=0.5$时。另外，如果计算损失函数关于模型参数的梯度的范数，可以得到图(b)，也是可以通过图中的对比得出，ERM对输入的变化更加敏感，也就是有可能会因为小的扰动导致大的预测变化，而使用mixup方法后的模型对输入的扰动更不敏感，表明这个模型的决策更加稳定。

![ERM与mixup方法对比](https://gitee.com/fbanhua/figurebed/raw/master/images/20250328144918570.png)

## 基本实验

论文在不同的数据集上进行了实验，具体的数据集信息如下。

| 数据集名称      | 数据集类别                     | 类别数 |
| --------------- | ------------------------------ | ------ |
| ImageNet-2012   | 图像分类                       | 1,000  |
| CIFAR-10        | 图像分类                       | 10     |
| CIFAR-100       | 图像分类                       | 100    |
| Google Commands | 语音识别分类                   | 30     |
| UCI Abalone     | 表格数据（年龄预测转换为分类） | 3      |
| UCI Arcene      | 表格数据（分类）               | 2      |
| UCI Arrhythmia  | 表格数据（分类）               | 16     |
| UCI Htru        | 表格数据（分类）               | 2      |
| UCI Iris        | 表格数据（分类）               | 3      |
| UCI Phishing    | 表格数据（分类）               | 2      |

首先是在ImageNet-2012数据集（图像分类）上的结果，可以看到，在使用相同模型的情况下，使用了mixup数据增强后的误差率均比原模型要低（其中Top1误差表示日常理解中的预测的错误率，Top5误差则是基于模型预测概率前5的类别中包含正确类别准确率所对应的错误率）。另外，论文发现**模型容量越大、训练轮数越多，mixup方法的提升越明显**。

![Imagenet2012结果](https://gitee.com/fbanhua/figurebed/raw/master/images/20250328150850635.png)

在CIFAR-10和CIFAR-100数据集（图像分类）上的效果如下，也是使用mixup方法后误差显著降低。

![CIFAR结果](https://gitee.com/fbanhua/figurebed/raw/master/images/20250328151555293.png)

在Google Commands数据集（语音分类）中，论文先将这些语音样本转换成声谱图之后再使用mixup数据增强方法。在这个实验中，mixup方法在简单模型上的效果变差了，但是在复杂模型（参数量更大的模型）上效果变好了。

![Google commands dataset](https://gitee.com/fbanhua/figurebed/raw/master/images/20250328153627808.png)

## 关于损坏标签的记忆

如果模型只是单纯的对训练集中的样本进行记忆（或者说过拟合），那么当训练集中出现标签错误的样本时就会导致模型在实际应用时产生较大的误差。为了研究mixup对这个问题的改善，论文继续使用CIFAR-10数据集，并采取了公开的**随机损坏标签**数据集实现：分别有20%、50%和80%三种损坏概率将训练集中的样本标签替换为随机噪声。

根据结果可以发现，**仅使用ERM**的方法会在损坏标签上出现严重的过拟合；**使用dropout方法**（在训练过程中随机丢弃神经元的输出）后可以有效缓解过拟合；而**使用较大的$\alpha$参数的mixup方法**在best轮的模型和last轮的模型均优于dropout方法；**结合使用mixup和dropout方法**的结果是最好的，表明这两种方法是兼容的。

![随机损坏标签](https://gitee.com/fbanhua/figurebed/raw/master/images/20250328155149805.png)

## 在对抗样本上的能力

对抗样本是指在样本上添加一些细微的扰动来降低模型的性能。为了研究mixup对对抗样本的分辨能力，论文在ImageNet-2012数据集上训练了三个ResNet-101模型：其中两个基于ERM方法进行训练，第三个使用mixup方法训练。

论文采用了两种对抗样本的生成方法：基于模型自身生成对抗样本（**白盒**方法）和基于另一个独立模型生成对抗样本（**黑盒**方法），这里的另一个独立模型就是前面两个基于ERM方法训练的模型的其中一个，另一个就是和mixup方法进行对比的模型。

可以得到的结果如下，使用mixup方法训练的模型对单步攻击（FGSM）的稳定性显著提升，但对迭代攻击（I-FGSM）仍敏感。

![对抗样本结果](https://gitee.com/fbanhua/figurebed/raw/master/images/20250328160755084.png)

## 非图像数据集上的性能

论文在六个UCI的表格数据上进行了测试，使用的模型仅两层全连接层。发现mixup提高了6个数据集中4个的平均测试误差，并且总体从未低于ERM方法。

![UCI数据集结果](https://gitee.com/fbanhua/figurebed/raw/master/images/20250328162745366.png)

## 在对抗生成网络(GANs)中的帮助

GANs由两个模型构成：生成器(generator)和判别器(discriminator)，生成器用于生成模拟真实样本的数据，判别器用于分类真实样本和生成样本，两者成对抗关系进行训练。论文认为，mixup可以稳定GANs的训练，通过判别器的平滑决策边界保证生成器具有稳定的梯度来源。

论文使用了一个简单的生成数据集来对比基于ERM方法和mixup方法的GANs，得到的结果如下。这个结果也符合前面提到的：训练轮数越多，mixup方法的提升越多。而基于ERM方法增加训练轮数的结果是无法正常生成。

![GANs](https://gitee.com/fbanhua/figurebed/raw/master/images/20250330101637413.png)

## 消融实验

表格中对比的基础方法是ERM，右边的“Modified”列打钩表示对比ERM方法有不同，打叉表示和ERM方法一致。

首先是**基础的mixup方法**，这里的AC表示在所有类别的样本中进行混合，RP表示混合方法是随机样本对混合，KNN表示是只和“相近”的样本进行混合。

然后是**基于mixup方法（所有类别混合+随机样本对）的中间层特征混合**，这里是指将中间层输出的特征混合后继续输入模型，然后基于混合后的标签进行损失函数的反向传播。

接着是**只对特征向量进行加权混合**，这里依然是AC表示跨类别，SC表示同类别内，KNN表示只使用最近的指定数量样本进行混合，RP表示随机样本对进行混合。

而**label smoothing方法**（标签软化）是指在独热编码中对错误类别的概率赋予一定的值（原本为0，$\epsilon $为对应赋予的权重），正确类别的概率则为1减去所有错误类别的概率，这一方法中只对标签进行修改，不改变输入样本的特征向量。

基于这个还有**mix inputs + label smoothing的方法**，这里就是先对特征向量进行加权混合，然后再使用label smoothing方法对标签向量进行软化。

最后，另一个对比方法就是往输入特征向量中**添加高斯噪声**，不改变标签向量。

最后，论文认为这个消融实验可以说明mixup方法是**效果最好的数据增强方法**，并且**具有一定的正则化效果**，这可以从对传统的 ERM 中较大的 weight decay 更有效，而对于 mixup 这类已经具备正则化效果的数据增强方法，则倾向于使用较小的 weight decay可以看出。（较大的 weight decay表示较强的正则化）

![消融实验结果](https://gitee.com/fbanhua/figurebed/raw/master/images/20250330155345975.png)

## 总结

论文认为，现有的数据增强方法中，人工往数据中**添加噪声**的方法（图像中的旋转、语音中的噪声注入）对相关邻域有严重的知识依赖；一些**样本插值**的方法只考虑到样本的特征变化没考虑到插值样本的标签变化（把mixup看做是特征和标签的同时插值）；而**对标签进行正则化**的方法（label smoothing）则没有建立特征和标签间的关系，独立于输入特征。而这篇论文中提出的mixup方法则是结合了上面方法的优点，避免了缺点。另外，论文推测**增加模型的参数可以降低模型对$\alpha$参数的敏感性**。